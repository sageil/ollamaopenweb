services:
  # ollama:
  #   # Uncomment below for GPU support
  #   # deploy:
  #   #   resources:
  #   #     reservations:
  #   #       devices:
  #   #         - driver: nvidia
  #   #           count: 1
  #   #           capabilities:
  #   #             - gpu
  #   volumes:
  #     - ollama:/root/.ollama
  #   ports:
  #     - 11434:11434
  #   container_name: ollama
  #   pull_policy: always
  #   tty: true
  #   restart: unless-stopped
  #   image: ollama/ollama:latest

  ollama-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: ollama-webui2
    volumes:
      - open-webui:/app/backend/data
    ports:
      - 3000:8080
    environment:
      - "OLLAMA_API_BASE_URL=http://localhost:11434"
    extra_hosts:
      - host.docker.internal:host-gateway
    restart: unless-stopped

volumes:
  open-webui: {}
